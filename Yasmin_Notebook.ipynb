{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dc2ONrUUi87F"
   },
   "source": [
    "# Usage of TweetEval and Twitter-specific RoBERTa models\n",
    "\n",
    "In this notebook we show how to perform tasks such as masked language modeling, computing tweet similarity or tweet classificationo using our Twitter-specific RoBERTa models.\n",
    "\n",
    "- Paper: [_TweetEval_ benchmark (Findings of EMNLP 2020)](https://arxiv.org/pdf/2010.12421.pdf)\n",
    "- Authors: Francesco Barbieri, Jose Camacho-Collados, Luis Espinosa-Anke and Leonardo Neves.\n",
    "- [Github](https://github.com/cardiffnlp/tweeteval)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J-FFwHN0jj3O"
   },
   "source": [
    "## Preliminaries\n",
    "\n",
    "We define a function to normalize a tweet to the format we used for TweetEval. Note that preprocessing is minimal (replacing user names by `@user` and links by `http`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gKE0LowmYpTY"
   },
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    new_text = []\n",
    "    for t in text.split(\" \"):\n",
    "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "        t = 'http' if t.startswith('http') else t\n",
    "        new_text.append(t)\n",
    "    return \" \".join(new_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JJ480-5vkRNR"
   },
   "source": [
    "We only need to install one dependnecy: the `transformers` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.executable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "42j4PR6vYrfg",
    "outputId": "0e806f16-5f58-4afa-e0d7-94d0bccaadc6"
   },
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "us58XJGyYpTS"
   },
   "source": [
    "## Computing Tweet Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AZq3Q5kKYpTg",
    "outputId": "a2d34be8-eda7-4e47-c281-ddf72fd18ea6"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, TFAutoModel\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "from collections import defaultdict\n",
    "\n",
    "MODEL = \"cardiffnlp/twitter-roberta-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "model = AutoModel.from_pretrained(MODEL)\n",
    "\n",
    "def get_embedding(text):\n",
    "  text = preprocess(text)\n",
    "  encoded_input = tokenizer(text, return_tensors='pt')\n",
    "  features = model(**encoded_input)\n",
    "  features = features[0].detach().cpu().numpy() \n",
    "  features_mean = np.mean(features[0], axis=0) \n",
    "  return features_mean\n",
    "\n",
    "query = \"The book was awesome\"\n",
    "\n",
    "tweets = [\"I just ordered fried chicken üê£\", \n",
    "          \"The movie was great\", \n",
    "          \"What time is the next game?\", \n",
    "          \"Just finished reading 'Embeddings in NLP'\"]\n",
    "\n",
    "d = defaultdict(int)\n",
    "for tweet in tweets:\n",
    "  sim = 1-cosine(get_embedding(query),get_embedding(tweet))\n",
    "  d[tweet] = sim\n",
    "\n",
    "print('Most similar to: ',query)\n",
    "print('----------------------------------------')\n",
    "for idx,x in enumerate(sorted(d.items(), key=lambda x:x[1], reverse=True)):\n",
    "  print(idx+1,x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oI6B4lOekYeJ"
   },
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0HALJ3ULdAaK",
    "outputId": "d261cb64-02fb-42de-ab67-8b74cd754a9d"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, TFAutoModel\n",
    "import numpy as np\n",
    "\n",
    "MODEL = \"cardiffnlp/twitter-roberta-base\"\n",
    "text = \"Good night üòä\"\n",
    "text = preprocess(text)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "# Pytorch\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "model = AutoModel.from_pretrained(MODEL)\n",
    "features = model(**encoded_input)\n",
    "features = features[0].detach().cpu().numpy() \n",
    "features_mean = np.mean(features[0], axis=0) \n",
    "#features_max = np.max(features[0], axis=0)\n",
    "\n",
    "# # Tensorflow\n",
    "# encoded_input = tokenizer(text, return_tensors='tf')\n",
    "# model = TFAutoModel.from_pretrained(MODEL)\n",
    "# features = model(encoded_input)\n",
    "# features = features[0].numpy()\n",
    "# features_mean = np.mean(features[0], axis=0) \n",
    "# #features_max = np.max(features[0], axis=0)\n",
    "\n",
    "features_mean.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ac8A2XgzYpTk"
   },
   "source": [
    "## Masked language modeling\n",
    "\n",
    "Use Twitter-RoBERTA-base to predict words in context using the `fill-mask` pipeline in `transformers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-pCzp7DIYpTl",
    "outputId": "2dccfcf7-6fe8-492e-8943-cfe1b51d87fa"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "MODEL = \"cardiffnlp/twitter-roberta-base\"\n",
    "fill_mask = pipeline(\"fill-mask\", model=MODEL, tokenizer=MODEL)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "def print_candidates():\n",
    "    for i in range(5):\n",
    "        token = tokenizer.decode(candidates[i]['token'])\n",
    "        score = np.round(candidates[i]['score'], 4)\n",
    "        print(f\"{i+1}) {token} {score}\")\n",
    "\n",
    "texts = [\n",
    " \"I am so <mask> üòä\",\n",
    " \"I am so <mask> üò¢\" \n",
    "]\n",
    "for text in texts:\n",
    "    t = preprocess(text)\n",
    "    print(f\"{'-'*30}\\n{t}\")\n",
    "    candidates = fill_mask(t)\n",
    "    print_candidates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Is2p3X9iYpTq"
   },
   "source": [
    "## Use TweetEval Classifiers\n",
    "\n",
    "We currently provide the following fine-tuned models for different tweet classification tasks:\n",
    "\n",
    "- emoji prediction (`emoji`)\n",
    "- emotion detection (`emotion`)\n",
    "- hate speech detection (`hate`)\n",
    "- irony detection (`irony`)\n",
    "- offensive language identification (`offensive`)\n",
    "- sentiment analysis (`sentiment`)\n",
    "- _(coming soon)_ stance detection (`stance`) with 5 targets (`abortion`, `atheism`, `climate`, `feminist`, `hillary`), for example: `stance-abortion`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 213,
     "referenced_widgets": [
      "e0847fcebda84c4d8b00b27234d35bc4",
      "8c7d3e2bd2714e929d6859e15e83003f",
      "9655bb1c674342cfb3ec38378a376729",
      "a5c3c70d67124cd09894d3b52dc33e16",
      "88a7b0479e8c40b6aea4084f8039d4f6",
      "41b12ad69a684cb597594e60355253ae",
      "e7aee59723a5469a9691322d6e124226",
      "683e01b6cd98402783fe062f75177b68",
      "f9c4c6ab3ba449e199d7035bff07a5db",
      "7ede88ead0d449a18ca343d47b723e87",
      "f55b841175a54f7ea34306caf6d775cd",
      "e2f226a69f704168bbbc6fca9df15e23",
      "91abbefb5e734d47a01158aebf46d7ce",
      "df23412503a1413aadacd254b573c315",
      "7a80cfb9bfda407c9ee1b9bd8b11a2f5",
      "b2d0e13e096c49cd9e25bb8a526b1ef7",
      "e958eee9b3db48abae24a1f8be1b5d91",
      "498473ac68aa48c1ba4d5578543a8e2b",
      "c2d4ce2a084b4f19bf7ab624c0a853ea",
      "75528dba634e465992e5b52107f19032",
      "7de2e7e4e8f44dccabc908334f7d1e21",
      "fd2b26c80b8e4807a595ce1ff9881623",
      "eab919bdf6cb499b90a14c297bbc94dc",
      "3b1773e755ca402f98b5033ed4af1b7a",
      "2151898d3e1c47de9e681c8b7b8779e4",
      "6e236c3f5e084a108811f1e6a32cb990",
      "627335e621884fb6957b7b6731339148",
      "02f91bc3545b48808e4812d13f480875",
      "2b6e1410b17543afb5dbf619666e83bd",
      "0a1beafebb954a498d7a1cccc7cee445",
      "db546face47140a59dcfc21539492c51",
      "25a58cab43ef453d8b9de797925f32fa"
     ]
    },
    "id": "F0xUNJwSYpTq",
    "outputId": "858885f3-cc50-48ec-9b57-db533a98bb52"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "import csv\n",
    "import urllib.request\n",
    "\n",
    "task='offensive'\n",
    "#task = 'hate'\n",
    "MODEL = f\"cardiffnlp/twitter-roberta-base-{task}\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g8_fEezTYpTu"
   },
   "outputs": [],
   "source": [
    "# download label mapping\n",
    "\n",
    "mapping_link = f\"https://github.com/cardiffnlp/tweeteval/blob/main/datasets/{task}/mapping.txt\"\n",
    "with urllib.request.urlopen(mapping_link) as f:\n",
    "    html = f.read().decode('utf-8').split(\"\\n\")\n",
    "    csvreader = csv.reader(html, delimiter='\\t')\n",
    "labels = [row[1] for row in csvreader if len(row) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "b9024a2af0984f0a99d7542077d5d183",
      "2adcfba8e99b4ceeae8cea6c4060c58d",
      "77dad5843dbc43eab577b4cef273228d",
      "d647beae5bff452ebc5e4dbc9974d63e",
      "7045c78b1b164511b62e20bec70f1639",
      "ca9214bf42bf49d99bcec60cc60c6e9c",
      "6d41343536e54dd3804ace50e15b7953",
      "1e5e9065e1ed4adbb1b451672ca4e793"
     ]
    },
    "id": "YQ_EGuYaYpTy",
    "outputId": "ed18d44d-b997-48d3-b894-a8d6482e3cad"
   },
   "outputs": [],
   "source": [
    "# PT\n",
    "#model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "\n",
    "#text = \"bitch\"\n",
    "#text = preprocess(text)\n",
    "#encoded_input = tokenizer(text, return_tensors='pf')\n",
    "#output = model(**encoded_input)\n",
    "#scores = output[0][0].detach().numpy()\n",
    "#scores = softmax(scores)\n",
    "import tensorflow as tf\n",
    "# # TF\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "\n",
    "text = \"Good night üòä\"\n",
    "encoded_input = tokenizer(text, return_tensors='tf')\n",
    "output = model(encoded_input)\n",
    "scores = output[0][0].numpy()\n",
    "scores = softmax(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mUG9EAWIYpT1",
    "outputId": "b59e5aa2-878a-4773-c676-3a9049a83307"
   },
   "outputs": [],
   "source": [
    "ranking = np.argsort(scores)\n",
    "ranking = ranking[::-1]\n",
    "for i in range(scores.shape[0]):\n",
    "    l = labels[ranking[i]]\n",
    "    s = scores[ranking[i]]\n",
    "    print(f\"{i+1}) {l} {np.round(float(s), 4)}\")"
   ]
  },
  {
   "source": [
    "# TASK 1.1: CLEANING DATA SET AND TOKENIZATION\n",
    "_ removing any unwanted characters and splitting text files into words _\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer \n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "import operator\n",
    "from collections import Counter\n",
    "import collections\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import pathlib\n",
    "import string\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.tokenize import word_tokenize\n",
    "ps = PorterStemmer()\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 1,
   "outputs": []
  },
  {
   "source": [
    "## Functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### a) Tokenization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize text file into sentences\n",
    "from nltk import sent_tokenize\n",
    "# nltk.download('punkt')\n",
    "\n",
    "def token_sentences(text):\n",
    "    sentences = sent_tokenize(text)   \n",
    "    return sentences\n",
    "\n",
    "\n",
    "# tokenize text file into words\n",
    "def tokenization(text):\n",
    "\n",
    "    tokens = word_tokenize(text[1:])\n",
    "    words = [token.lower() for token in tokens if token.isalpha()]\n",
    "    \n",
    "    return words\n",
    "\n"
   ]
  },
  {
   "source": [
    "### a) Vocabulary\n",
    "_ step 1: Tokenize the words into characters in the corpus and append </w> at the end of every word_"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "_Looking through vocabulary lists can help you find problems\n",
    "(especially tokens that only occur once or twice)._"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting a dataframe into a single list \n",
    "#text is split into words defined by their space inbetween\n",
    "#words are inserted into list \n",
    "\n",
    "def words_list(text):\n",
    "    #words are inserted into list \n",
    "    corpus=[]\n",
    "    for row in text:\n",
    "        tokens = row[0].split(\" \")\n",
    "        for token in tokens:\n",
    "            corpus.append(token)\n",
    "    \n",
    "    \n",
    "    def vocabulary_list(corpus):\n",
    "        #initlialize the vocabulary\n",
    "        vocab = list(set(\" \".join(corpus)))\n",
    "        vocab.remove(' ')\n",
    "        return vocab\n",
    "      \n",
    "    \n",
    "    def split_words_char(corpus):\n",
    "        #split the word into characters\n",
    "        corpus = [\" \".join(token) for token in corpus]\n",
    "\n",
    "        #appending </w>\n",
    "        corpus=[token+' </w>' for token in corpus]\n",
    "        return corpus\n",
    "        \n",
    "    x,y = split_words_char(corpus), vocabulary_list(corpus)\n",
    "\n",
    "\n",
    "    return x,y\n",
    "\n",
    "\n",
    "# Creating the Bag of Words model\n",
    "def bag_of_words(text):\n",
    "    word2count = {}\n",
    "    for data in text:\n",
    "        words = nltk.word_tokenize(data)\n",
    "        for word in words:  \n",
    "            if word not in word2count.keys():\n",
    "                word2count[word] = 1\n",
    "            else:\n",
    "                word2count[word] += 1\n",
    "    return word2count   \n",
    "\n",
    "# Frequency of words in BAG\n",
    "def freq_words(word2count):\n",
    "    import heapq\n",
    "    freq_words = heapq.nlargest(100, word2count, key=word2count.get)\n",
    "    return freq_words\n"
   ]
  },
  {
   "source": [
    "### b) Cleaning"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing punctuations\n",
    "# convert all words into lower cases\n",
    "# remove stop words\n",
    "\n",
    "def preprocess_text(words):\n",
    "    #delete punctuations\n",
    "    punctuation_table = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "    #convert all words into lower cases\n",
    "    words = [word.translate(punctuation_table).lower() for word in words]\n",
    "    \n",
    "    #remove stopwords\n",
    "    from nltk.corpus import stopwords\n",
    "    stop_words = stopwords.words('english')\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    return words\n",
    "\n",
    "\n",
    "# cleaning sentences within data frame\n",
    "def  clean_text(df, text_field):\n",
    "    df[text_field] = df[text_field].str.lower()\n",
    "    df[text_field] = df[text_field].apply(lambda elem: re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \" \", elem))  \n",
    "    return df\n",
    "\n",
    "\n",
    "# remove only punctuations\n",
    "def del_punctuations(words):\n",
    "\n",
    "    punctuation_table = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "    words = [word.translate(punctuation_table).lower() for word in words]\n",
    "    return words\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "# remove stop words\n",
    "def stop_words(words):\n",
    "    stop_words = stopwords.words('english')\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    return words\n",
    "\n",
    "#nltk.download('wordnet')\n",
    "#lemmatization of words\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def word_lemmatizer(text):\n",
    "    lem_text = [WordNetLemmatizer().lemmatize(i) for i in text]\n",
    "    return lem_text\n",
    "\n",
    "def word_stemmer(text):\n",
    "    stem_text = [PorterStemmer().stem(i) for i in text]\n",
    "    return stem_text\n"
   ]
  },
  {
   "source": [
    "### c) Basic statistics"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of tokens, sentences, average tokens, total unique tokens, total number of tokens after cleaning\n",
    "\n",
    "def basic_statistics(text):\n",
    "    sents = nltk.sent_tokenize(text)\n",
    "    tokens = word_tokenize(text[1:])\n",
    "    words = [token.lower() for token in tokens if token.isalpha()]\n",
    "    average_tokens = round(len(words)/len(sents))\n",
    "    unique_tokens = set(words)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    final_tokens = []\n",
    "    for each in words:\n",
    "        if each not in stop_words:\n",
    "            final_tokens.append(each) \n",
    "    \n",
    "    return len(tokens), len(sents), average_tokens, len(unique_tokens), len(final_tokens)\n",
    "\n",
    "\n",
    "#returns frequency of each word\n",
    "def word_frequency(words):\n",
    "    frequency_words = collections.Counter(words)\n",
    "    \n",
    "    #convert counter object to dictionary\n",
    "    frequency_words_dict = dict(frequency_words)\n",
    "    res = dict(sorted(frequency_words_dict.items(), key=lambda item: item[1]))\n",
    "    return res\n",
    "\n",
    "#returns top 20 most common words\n",
    "def top_20_most_common_words(freq_words):\n",
    "    res = dict(Counter(freq_words).most_common(20))\n",
    "    return res"
   ]
  },
  {
   "source": [
    "## A) Tokenization of text files: Offensive"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Read in txt files: offensive/val_text.txt"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['user', '@user', 'WiiU', 'is', 'not', 'even', 'a', 'real', 'console.', '@user', '@user', '@user', 'If', 'he', 'is', 'from', 'AZ', 'I', 'would', 'put', 'my', 'money', 'on', 'sex', 'with', 'underage', 'kids.', '@user', 'I', 'thought', 'Canada', 'had', 'strict', 'gun', 'control.', 'Help', 'me', 'understand', 'what', 'is', 'happening.', '@user', '@user', '@user', '@user', '@user', '@user', '@user', '@user', '@user', '@user', '@user', '@user', '@user', '@user', '@user', '@user', 'Following', 'all', '#Maga', 'patriots', 'please', 'follow', 'back', 'üëç', '#LionsDen', 'ü¶Å', '#MAGA2KAG', 'üá∫üá∏', '1', 'Minute', 'of', 'Truth:', 'Gun', 'Control', 'via', '@user', '@user', '@user', '@user', 'We', 'could', 'help', 'if', 'you', 'are', 'London', 'based', 'üòä', '@user', '@user', 'There', 'r', '65', 'million', 'that', 'can', 'sign', 'to', 'the']\n"
     ]
    }
   ],
   "source": [
    "file_path_val = pathlib.Path('datasets/offensive/val_text.txt')\n",
    "\n",
    "with open(file_path_val, 'r') as f:\n",
    "    text_val = f.read()       \n",
    "    f.close()\n",
    "    \n",
    "words_val_txt = text_val[1:].split()\n",
    "print(words_val_txt[:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['user', 'Bono...', 'who', 'cares.', 'Soon', 'people', 'will', 'understand', 'that', 'they', 'gain', 'nothing', 'from', 'following', 'a', 'phony', 'celebrity.', 'Become', 'a', 'Leader', 'of', 'your', 'people', 'instead', 'or', 'help', 'and', 'support', 'your', 'fellow', 'countrymen.', '@user', 'Eight', 'years', 'the', 'republicans', 'denied', 'obama‚Äôs', 'picks.', 'Breitbarters', 'outrage', 'is', 'as', 'phony', 'as', 'their', 'fake', 'president.', '@user', 'Get', 'him', 'some', 'line', 'help.', 'He', 'is', 'gonna', 'be', 'just', 'fine.', 'As', 'the', 'game', 'went', 'on', 'you', 'could', 'see', 'him', 'progressing', 'more', 'with', 'his', 'reads.', 'He', 'brought', 'what', 'has', 'been', 'missing.', 'The', 'deep', 'ball', 'presence.', 'Now', 'he', 'just', 'needs', 'a', 'little', 'more', 'time', '@user', '@user', 'She', 'is', 'great.', 'Hi', 'Fiona!', '@user']\n"
     ]
    }
   ],
   "source": [
    "file_path_train = pathlib.Path('datasets/offensive/train_text.txt')\n",
    "\n",
    "\n",
    "with open(file_path_train, 'r') as f:\n",
    "    text_train = f.read()       \n",
    "    f.close()\n",
    "    \n",
    "words_train_txt = text_train[1:].split()\n",
    "\n",
    "print(words_train_txt[:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['ibelieveblaseyford', 'is', 'liar', 'she', 'is', 'fat', 'ugly', 'libreal', '#snowflake', 'she', 'sold', 'her', 'herself', 'to', 'get', 'some', 'cash', '!!', 'From', 'dems', 'and', 'Iran', '!', 'Why', 'she', 'spoke', 'after', '#JohnKerryIranMeeting', '?', '@user', '@user', '@user', 'I', 'got', 'in', 'a', 'pretty', 'deep', 'debate', 'with', 'my', 'friend', 'and', 'she', 'told', 'me', 'that', 'latinos', 'for', 'Trump', 'and', 'blacks', 'for', 'Trump', 'were', 'paid', 'supporters', 'üòÇ', 'then', 'I', 'said', 'you', 'mean', 'antifa', 'are', 'paid', 'domestic', 'terrorist', 'and', 'she', 'said', 'No', 'they', 'are', 'anti-fascist', 'then', 'I', 'said', 'they', 'are', 'the', 'fascist', 'are', 'you', 'kidding', 'me?!', '...if', 'you', 'want', 'more', 'shootings', 'and', 'more', 'death,', 'then', 'listen', 'to', 'the', 'ACLU,', 'Black']\n"
     ]
    }
   ],
   "source": [
    "file_path_test = pathlib.Path('datasets/offensive/test_text.txt')\n",
    "\n",
    "\n",
    "with open(file_path_test, 'r') as f:\n",
    "    text_test = f.read()       \n",
    "    f.close()\n",
    "    \n",
    "words_test_txt = text_test[1:].split()\n",
    "\n",
    "print(words_test_txt[:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of words in tokenization for val_text: 30416\nNumber of words in tokenization for val_train: 258224\nNumber of words in tokenization for val_test: 19619\n"
     ]
    }
   ],
   "source": [
    "# total number of tokenization of words for each variable\n",
    "token_val = tokenization(text_val)\n",
    "token_train = tokenization(text_train)\n",
    "token_test = tokenization(text_test)\n",
    "\n",
    "print(f'Number of words in tokenization for val_text: {len(token_val)}')\n",
    "print(f'Number of words in tokenization for val_train: {len(token_train)}')\n",
    "print(f'Number of words in tokenization for val_test: {len(token_test)}')"
   ]
  },
  {
   "source": [
    "### Tokenize text file into sentences"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['@user @user WiiU is not even a real console.', '@user @user @user If he is from AZ I would put my money on sex with underage kids.', '@user I thought Canada had strict gun control.', 'Help me understand what is happening.', \"@user @user @user @user @user @user @user @user @user @user @user @user @user @user @user @user Following all #Maga patriots please follow back üëç  #LionsDen ü¶Å  #MAGA2KAG üá∫üá∏ \\n1 Minute of Truth: Gun Control via @user \\n@user @user @user We could help if you are London based üòä \\n@user @user There r 65 million that can sign to the affect that they didn't vote for an asshole.\"]\n['@user Bono... who cares.', 'Soon people will understand that they gain nothing from following a phony celebrity.', 'Become a Leader of your people instead or help and support your fellow countrymen.', '@user Eight years the republicans denied obama‚Äôs picks.', 'Breitbarters outrage is as phony as their fake president.']\n['#ibelieveblaseyford is liar she is fat ugly libreal #snowflake she sold her  herself to get some cash !!', 'From dems and Iran  !', 'Why she spoke after  #JohnKerryIranMeeting ?', '@user @user @user I got in a pretty deep debate with my friend and she told me that latinos for Trump and blacks for Trump were paid supporters üòÇ then I said you mean antifa are paid domestic terrorist and she said No they are  anti-fascist then I said they are the fascist are you kidding me?!', '...if you want more shootings and more death, then listen to the ACLU, Black Lives Matter, or Antifa.']\n"
     ]
    }
   ],
   "source": [
    "sentences_val_txt = token_sentences(text_val)  \n",
    "sentences_train_txt = token_sentences(text_train)\n",
    "sentences_test_txt = token_sentences(text_test)   \n",
    "\n",
    "print(sentences_val_txt[:5])\n",
    "print(sentences_train_txt[:5])\n",
    "print(sentences_test_txt[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of sentences for val_text: 2124\nNumber of sentences for val_train: 18122\nNumber of sentences for val_test: 1214\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of sentences for val_text: {len(sentences_val_txt)}')\n",
    "print(f'Number of sentences for val_train: {len(sentences_train_txt)}')\n",
    "print(f'Number of sentences for val_test: {len(sentences_test_txt)}')"
   ]
  },
  {
   "source": [
    "### Vocabulary list for offensive text files\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Vocabulary for text_val.txt:\n ['üôÇ', 'üèº', 'w', 'üëç', 'X', 'A', 'c', 'üôâ', 'O', 'üáß', '‚úî', 'üëÇ', '‚Äì', 'ü•Ä', 'üèª', 'M', '‚ù§', 'üá¨', '3', 'v'] \n\nVocabulary for text_train.txt:\n ['X', 'üíé', 'üôÉ', 'Áöø', 'ü•Ä', 'M', 'v', 'üí∞', 'üïä', '¬∫', 'üíµ', 'y', 'üëΩ', '¬∑', 'üßê', 'ü§Æ', 'i', 'üñï', 'üê∑', 'ü§ô'] \n\nVocabulary for text_test.txt:\n ['üîµ', 'üèº', 'w', 'X', 'üëç', 'A', 'üèÅ', 'üò•', 'c', 'üíé', 'O', 'üáß', '‚Äì', '‚úî', 'üôÉ', 'üèª', 'M', '‚ù§', 'üíì', '3'] \n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "corpus_val, vocab_val = words_list(text_val)\n",
    "corpus_train, vocab_train = words_list(text_train)\n",
    "corpus_test, vocab_test = words_list(text_test)\n",
    "\n",
    "print('Vocabulary for text_val.txt:\\n', vocab_val[:20],'\\n')\n",
    "print('Vocabulary for text_train.txt:\\n', vocab_train[:20],'\\n')\n",
    "print('Vocabulary for text_test.txt:\\n', vocab_test[:20],'\\n')"
   ]
  },
  {
   "source": [
    "***"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## B) Cleaning: Pre-processing text file\n",
    "\n",
    "_\n",
    "1. remove puncuations \n",
    "2. convert all words into lower case\n",
    "2. remove stop words\n",
    "_"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Remove punctuations from txt files\n",
    "_ meaning signs, spacing and other disturbing features. Alle words are then turned into lower cases_"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "val_text.txt:\n ['user', 'user', 'wiiu', 'is', 'not', 'even', 'a', 'real', 'console', 'user'] \n\n\nval_train.txt:\n ['user', 'bono', 'who', 'cares', 'soon', 'people', 'will', 'understand', 'that', 'they'] \n\n\nval_test.txt:\n ['ibelieveblaseyford', 'is', 'liar', 'she', 'is', 'fat', 'ugly', 'libreal', 'snowflake', 'she'] \n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cleaned_val_words = del_punctuations(words_val_txt)\n",
    "cleaned_train_words = del_punctuations(words_train_txt)\n",
    "cleaned_test_words = del_punctuations(words_test_txt)\n",
    "\n",
    "# preview\n",
    "print('val_text.txt:\\n',cleaned_val_words[:10],'\\n')\n",
    "print('\\nval_train.txt:\\n',cleaned_train_words[:10], '\\n')\n",
    "print('\\nval_test.txt:\\n',cleaned_test_words[:10],'\\n')\n",
    "\n"
   ]
  },
  {
   "source": [
    "### Remove stop words and total number of each variable after cleaning"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of words after removing Stop Words: 17155\nNumber of words after removing Stop Words: 147302\nNumber of words after removing Stop Words: 11080\n"
     ]
    }
   ],
   "source": [
    "cleaned_val_words = stop_words(token_val)\n",
    "cleaned_train_words = stop_words(token_train)\n",
    "cleaned_test_words = stop_words(token_test)\n",
    "\n",
    "# display\n",
    "print(f'Number of words after removing Stop Words: {len(cleaned_val_words)}')\n",
    "print(f'Number of words after removing Stop Words: {len(cleaned_train_words)}')\n",
    "print(f'Number of words after removing Stop Words: {len(cleaned_test_words)}')\n"
   ]
  },
  {
   "source": [
    "***"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## C) Basic Statistics"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Function for calculating:\n",
    "\n",
    "1. Number of tokens\n",
    "2. Number of sentences\n",
    "3. Average tokens pr sentence\n",
    "4. Number of unique tokens\n",
    "5. Final number of tokens after removing stop words"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### text_val.txt"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#get number of: \n",
    "# total tokens\n",
    "# total sentences\n",
    "# average tokens\n",
    "# unique tokens \n",
    "# final number of tokens after removing stop words\n",
    "\n",
    "tokens_count, sents_count, avg_tokens, uni_tokens, final_tokens = basic_statistics(text_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The number of tokens is: 39409\nThe number of sentences is: 2124\nThe average number of tokens per sentence is: 14\nThe number of unique tokens are: 4961\nThe number of total tokens after removing stopwords are: 17155\n"
     ]
    }
   ],
   "source": [
    "print(f'The number of tokens is: {tokens_count}')\n",
    "print(f'The number of sentences is: {sents_count}')\n",
    "print(f'The average number of tokens per sentence is: {avg_tokens}')\n",
    "print(f'The number of unique tokens are: {uni_tokens}')\n",
    "print(f'The number of total tokens after removing stopwords are: {final_tokens}')"
   ]
  },
  {
   "source": [
    "### train_text.txt"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get number of: \n",
    "# total tokens\n",
    "# total sentences\n",
    "# average tokens\n",
    "# unique tokens \n",
    "# final number of tokens after removing stop words\n",
    "\n",
    "tokens_train, sents_train, avg_train, uni_train, final_train = basic_statistics(text_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The number of tokens is: 336416\nThe number of sentences is: 18122\nThe average number of tokens per sentence is: 14\nThe number of unique tokens are: 17111\nThe number of total tokens after removing stopwords are: 147302\n"
     ]
    }
   ],
   "source": [
    "print(f'The number of tokens is: {tokens_train}')\n",
    "print(f'The number of sentences is: {sents_train}')\n",
    "print(f'The average number of tokens per sentence is: {avg_train}')\n",
    "print(f'The number of unique tokens are: {uni_train}')\n",
    "print(f'The number of total tokens after removing stopwords are: { final_train}')"
   ]
  },
  {
   "source": [
    "### text_test.txt"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get number of: \n",
    "# total tokens\n",
    "# total sentences\n",
    "# average tokens\n",
    "# unique tokens \n",
    "# final number of tokens after removing stop words\n",
    "\n",
    "tokens_test, sents_test, avg_test, uni_test, final_test = basic_statistics(text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The number of tokens is: 26036\nThe number of sentences is: 1214\nThe average number of tokens per sentence is: 16\nThe number of unique tokens are: 4839\nThe number of total tokens after removing stopwords are: 11080\n"
     ]
    }
   ],
   "source": [
    "print(f'The number of tokens is: {tokens_test}')\n",
    "print(f'The number of sentences is: {sents_test}')\n",
    "print(f'The average number of tokens per sentence is: {avg_test}')\n",
    "print(f'The number of unique tokens are: {uni_test}')\n",
    "print(f'The number of total tokens after removing stopwords are: { final_test}')"
   ]
  },
  {
   "source": [
    "### Frequency \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "________________________________________________________________________________________________________________ \n\nFrequency of words in val_text:\n [('love', 31), ('great', 31), ('much', 32), ('democrats', 32), ('country', 32), ('could', 33), ('vote', 33), ('shit', 33), ('never', 35), ('believe', 35), ('way', 35), ('need', 36), ('say', 37), ('still', 38), ('time', 40), ('make', 40), ('go', 41), ('good', 42), ('even', 43), ('see', 44), ('right', 45), ('going', 47), ('us', 52), ('want', 55), ('would', 61), ('amp', 62), ('think', 68), ('trump', 69), ('one', 71), ('get', 73), ('know', 77), ('people', 89), ('maga', 98), ('conservatives', 107), ('like', 109), ('antifa', 118), ('control', 125), ('gun', 133), ('liberals', 137)] \n\n________________________________________________________________________________________________________________ \n\nFrequency of words in train_text:\n [('better', 237), ('vote', 237), ('well', 240), ('much', 249), ('left', 252), ('still', 262), ('make', 276), ('way', 278), ('really', 284), ('love', 290), ('back', 290), ('say', 292), ('even', 294), ('going', 312), ('see', 318), ('shit', 319), ('never', 325), ('need', 326), ('want', 329), ('go', 340), ('us', 345), ('time', 349), ('right', 409), ('good', 416), ('think', 483), ('would', 507), ('know', 557), ('trump', 565), ('one', 568), ('get', 586), ('amp', 615), ('people', 830), ('conservatives', 839), ('maga', 907), ('like', 995), ('antifa', 1047), ('control', 1095), ('gun', 1230), ('liberals', 1260)] \n\n________________________________________________________________________________________________________________ \n\nFrequency of words in test_text:\n [('chicago', 18), ('keep', 19), ('even', 19), ('years', 21), ('go', 21), ('really', 21), ('think', 22), ('life', 22), ('still', 22), ('always', 23), ('support', 24), ('shit', 24), ('time', 25), ('please', 25), ('way', 26), ('democrats', 26), ('need', 27), ('kavanaugh', 28), ('never', 28), ('see', 29), ('going', 30), ('new', 30), ('know', 31), ('good', 31), ('via', 33), ('want', 37), ('love', 38), ('us', 42), ('trump', 44), ('people', 47), ('one', 48), ('get', 51), ('maga', 57), ('gun', 64), ('control', 64), ('like', 65), ('antifa', 74), ('conservatives', 80), ('liberals', 81)] \n\n________________________________________________________________________________________________________________ \n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "freq_words_val = word_frequency(cleaned_val_words)\n",
    "freq_words_train = word_frequency(cleaned_train_words)\n",
    "freq_words_test = word_frequency(cleaned_test_words)\n",
    "\n",
    "# displays the final 40 elements from the back end of the list\n",
    "print('_'*112,'\\n')\n",
    "print('Frequency of words in val_text:\\n',list(freq_words_val.items())[-40:-1], '\\n')\n",
    "print('_'*112,'\\n')\n",
    "print('Frequency of words in train_text:\\n',list(freq_words_train.items())[-40:-1], '\\n')\n",
    "print('_'*112,'\\n')\n",
    "print('Frequency of words in test_text:\\n',list(freq_words_test.items())[-40:-1], '\\n')\n",
    "print('_'*112,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Top 20 in val_text.txt:\n {'user': 3455, 'liberals': 137, 'gun': 133, 'control': 125, 'antifa': 118, 'like': 109, 'conservatives': 107, 'maga': 98, 'people': 89, 'know': 77, 'get': 73, 'one': 71, 'trump': 69, 'think': 68, 'amp': 62, 'would': 61, 'want': 55, 'us': 52, 'going': 47, 'right': 45} \n\nTop 20 in val_train.txt:\n {'user': 29961, 'liberals': 1260, 'gun': 1230, 'control': 1095, 'antifa': 1047, 'like': 995, 'maga': 907, 'conservatives': 839, 'people': 830, 'amp': 615, 'get': 586, 'one': 568, 'trump': 565, 'know': 557, 'would': 507, 'think': 483, 'good': 416, 'right': 409, 'time': 349, 'us': 345} \n\nTop 20 in val_test.txt:\n {'user': 608, 'liberals': 81, 'conservatives': 80, 'antifa': 74, 'like': 65, 'gun': 64, 'control': 64, 'maga': 57, 'get': 51, 'one': 48, 'people': 47, 'trump': 44, 'us': 42, 'love': 38, 'want': 37, 'via': 33, 'know': 31, 'good': 31, 'going': 30, 'new': 30} \n\n"
     ]
    }
   ],
   "source": [
    "# top 20 of common words\n",
    "top_20_val = top_20_most_common_words(freq_words_val)\n",
    "top_20_train = top_20_most_common_words(freq_words_train)\n",
    "top_20_test = top_20_most_common_words(freq_words_test)\n",
    "\n",
    "print('Top 20 in val_text.txt:\\n',top_20_val, '\\n')\n",
    "print('Top 20 in val_train.txt:\\n',top_20_train,'\\n')\n",
    "print('Top 20 in val_test.txt:\\n',top_20_test,'\\n')"
   ]
  },
  {
   "source": [
    "***"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "_source:https://medium.com/vickdata/detecting-hate-speech-in-tweets-natural-language-processing-in-python-for-beginners-4e591952223 _"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training Set: (11916, 1) 11916\nTest Set: (860, 1) 860\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv('datasets/offensive/train_text.txt', header=None, skiprows = 0, names=['tweet'], sep='\\t', quoting=3)\n",
    "print(\"Training Set:\"% train.columns, train.shape, len(train))\n",
    "\n",
    "\n",
    "test = pd.read_csv('datasets/offensive/test_text.txt',header=None, skiprows = 0, names=['tweet'], sep='\\t', quoting=3)\n",
    "print(\"Test Set:\"% test.columns, test.shape, len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "index_train = [x for x in range(1, len(train.values)+1)]\n",
    "index_test = [x for x in range(1, len(test.values)+1)]\n",
    "\n",
    "train.insert(loc=0, column='id', value =index_train )\n",
    "test.insert(loc=0, column='id', value =index_test )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = pd.read_csv('datasets/offensive/train_labels.txt',header=None, skiprows = 0, names=['label'], sep='\\t', quoting=3)\n",
    "train_labels.insert(loc=0, column='id', value=index_train)\n",
    "\n",
    "test_labels = pd.read_csv('datasets/offensive/test_labels.txt',header=None, skiprows = 0, names=['label'], sep='\\t', quoting=3)\n",
    "test_labels.insert(loc=0, column='id', value =index_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test.merge(test_labels, on='id', how='left')\n",
    "train_df = train.merge(train_labels, on='id', how='left')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "          id                                              tweet  label\n",
       "0          1  @user Bono... who cares. Soon people will unde...      0\n",
       "1          2  @user Eight years the republicans denied obama...      1\n",
       "2          3  @user Get him some line help. He is gonna be j...      0\n",
       "3          4               @user @user She is great. Hi Fiona!       0\n",
       "4          5  @user She has become a parody unto herself? Sh...      1\n",
       "...      ...                                                ...    ...\n",
       "11911  11912   @user I wonder if they are sex traffic victims?       1\n",
       "11912  11913  @user Do we dare say he is... better than Nyjer?       0\n",
       "11913  11914                    @user No idea who he is. Sorry       0\n",
       "11914  11915  #Professor Who Shot Self Over Trump Says Gun C...      0\n",
       "11915  11916  @user @user @user Here your proof!  Our Africa...      1\n",
       "\n",
       "[11916 rows x 3 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>tweet</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>@user Bono... who cares. Soon people will unde...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>@user Eight years the republicans denied obama...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>@user Get him some line help. He is gonna be j...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>@user @user She is great. Hi Fiona!</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>@user She has become a parody unto herself? Sh...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>11911</th>\n      <td>11912</td>\n      <td>@user I wonder if they are sex traffic victims?</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>11912</th>\n      <td>11913</td>\n      <td>@user Do we dare say he is... better than Nyjer?</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>11913</th>\n      <td>11914</td>\n      <td>@user No idea who he is. Sorry</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>11914</th>\n      <td>11915</td>\n      <td>#Professor Who Shot Self Over Trump Says Gun C...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>11915</th>\n      <td>11916</td>\n      <td>@user @user @user Here your proof!  Our Africa...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>11916 rows √ó 3 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 149
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "      id                                              tweet  label\n",
       "0      1  #ibelieveblaseyford is liar she is fat ugly li...      1\n",
       "1      2  @user @user @user I got in a pretty deep debat...      0\n",
       "2      3  ...if you want more shootings and more death, ...      0\n",
       "3      4  Angels now have 6 runs. Five of them have come...      0\n",
       "4      5  #Travel #Movies and Unix #Fortune combined  Vi...      0\n",
       "..   ...                                                ...    ...\n",
       "855  856  #CNN irrationally argues 4 legalising #abortio...      0\n",
       "856  857  @user @user @user @user @user @user @user @use...      0\n",
       "857  858  #Conservatives don‚Äôt care what you post..it‚Äôs ...      1\n",
       "858  859  #antifa #Resist.. Trump is trying to bring wor...      0\n",
       "859  860  #Maine you need to face facts @user doesn‚Äôt re...      0\n",
       "\n",
       "[860 rows x 3 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>tweet</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>#ibelieveblaseyford is liar she is fat ugly li...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>@user @user @user I got in a pretty deep debat...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>...if you want more shootings and more death, ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>Angels now have 6 runs. Five of them have come...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>#Travel #Movies and Unix #Fortune combined  Vi...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>855</th>\n      <td>856</td>\n      <td>#CNN irrationally argues 4 legalising #abortio...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>856</th>\n      <td>857</td>\n      <td>@user @user @user @user @user @user @user @use...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>857</th>\n      <td>858</td>\n      <td>#Conservatives don‚Äôt care what you post..it‚Äôs ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>858</th>\n      <td>859</td>\n      <td>#antifa #Resist.. Trump is trying to bring wor...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>859</th>\n      <td>860</td>\n      <td>#Maine you need to face facts @user doesn‚Äôt re...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>860 rows √ó 3 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 150
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_clean = clean_text(test_df, 'tweet')\n",
    "train_clean = clean_text(train_df, 'tweet')"
   ]
  },
  {
   "source": [
    "https://towardsdatascience.com/text-cleaning-methods-for-natural-language-processing-f2fc1796e8c7\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   id                                              tweet  label\n",
       "0   1   ibelieveblaseyford is liar she is fat ugly li...      1\n",
       "1   2        i got in a pretty deep debate with my fr...      0\n",
       "2   3     if you want more shootings and more death  ...      0\n",
       "3   4  angels now have 6 runs  five of them have come...      0\n",
       "4   5   travel  movies and unix  fortune combined  vi...      0"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>tweet</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>ibelieveblaseyford is liar she is fat ugly li...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>i got in a pretty deep debate with my fr...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>if you want more shootings and more death  ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>angels now have 6 runs  five of them have come...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>travel  movies and unix  fortune combined  vi...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 152
    }
   ],
   "source": [
    "test_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   id                                              tweet  label\n",
       "0   1  bono cares soon people understand gain nothing...      0\n",
       "1   2  eight years republicans denied obama picks bre...      1\n",
       "2   3  get line help gonna fine game went could see p...      0\n",
       "3   4                                     great hi fiona      0\n",
       "4   5  become parody unto certainly taken heat well i...      1"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>tweet</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>bono cares soon people understand gain nothing...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>eight years republicans denied obama picks bre...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>get line help gonna fine game went could see p...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>great hi fiona</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>become parody unto certainly taken heat well i...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 153
    }
   ],
   "source": [
    "stop = stopwords.words('english')\n",
    "\n",
    "train_clean['tweet'] = train_clean['tweet'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "train_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   id                                              tweet  label\n",
       "0   1  ibelieveblaseyford liar fat ugly libreal snowf...      1\n",
       "1   2  got pretty deep debate friend told latinos tru...      0\n",
       "2   3  want shootings death listen aclu black lives m...      0\n",
       "3   4  angels 6 runs five come courtesy mike trout ho...      0\n",
       "4   5  travel movies unix fortune combined visit sali...      0"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>tweet</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>ibelieveblaseyford liar fat ugly libreal snowf...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>got pretty deep debate friend told latinos tru...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>want shootings death listen aclu black lives m...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>angels 6 runs five come courtesy mike trout ho...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>travel movies unix fortune combined visit sali...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 154
    }
   ],
   "source": [
    "stop = stopwords.words('english')\n",
    "test_clean['tweet'] = test_clean['tweet'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "\n",
    "test_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   id                                              tweet  label\n",
       "0   1  [ibelieveblaseyford, liar, fat, ugly, libreal,...      1\n",
       "1   2  [got, pretty, deep, debate, friend, told, lati...      0\n",
       "2   3  [want, shootings, death, listen, aclu, black, ...      0\n",
       "3   4  [angels, 6, runs, five, come, courtesy, mike, ...      0\n",
       "4   5  [travel, movies, unix, fortune, combined, visi...      0"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>tweet</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>[ibelieveblaseyford, liar, fat, ugly, libreal,...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>[got, pretty, deep, debate, friend, told, lati...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>[want, shootings, death, listen, aclu, black, ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>[angels, 6, runs, five, come, courtesy, mike, ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>[travel, movies, unix, fortune, combined, visi...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 155
    }
   ],
   "source": [
    "test_clean['tweet'] = test_clean['tweet'].apply(lambda x: word_tokenize(x))\n",
    "test_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_clean_offensive = test_clean.loc[test_clean['label'] == 1]\n",
    "test_clean_offensive.head()\n",
    "test_clean_offensive['tweet'].to_csv('test_cleaned_off_lang_df.csv',  quoting=csv.QUOTE_NONE, escapechar=' ') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   id                                              tweet  label\n",
       "1   2  [got, pretty, deep, debate, friend, told, lati...      0\n",
       "2   3  [want, shootings, death, listen, aclu, black, ...      0\n",
       "3   4  [angels, 6, runs, five, come, courtesy, mike, ...      0\n",
       "4   5  [travel, movies, unix, fortune, combined, visi...      0\n",
       "5   6  [naturephotography, nature, birds, wild, wisco...      0"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>tweet</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>[got, pretty, deep, debate, friend, told, lati...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>[want, shootings, death, listen, aclu, black, ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>[angels, 6, runs, five, come, courtesy, mike, ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>[travel, movies, unix, fortune, combined, visi...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>6</td>\n      <td>[naturephotography, nature, birds, wild, wisco...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 169
    }
   ],
   "source": [
    "import csv\n",
    "test_clean_nonoffensive = test_clean.loc[test_clean['label'] == 0]\n",
    "\n",
    "test_clean_nonoffensive['tweet'].to_csv('test_cleaned_non_off_lang_df.csv',  quoting=csv.QUOTE_NONE, escapechar=' ') \n",
    "\n",
    "test_clean_nonoffensive.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_csv = test_clean_nonoffensive.explode('tweet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['grown',\n",
       " 'ass',\n",
       " 'woman',\n",
       " 'probably',\n",
       " '10',\n",
       " 'years',\n",
       " 'older',\n",
       " 'currently',\n",
       " 'spreading',\n",
       " 'rumors',\n",
       " 'rather',\n",
       " 'talking',\n",
       " 'nice',\n",
       " 'work',\n",
       " 'got',\n",
       " 'satan']"
      ]
     },
     "metadata": {},
     "execution_count": 178
    }
   ],
   "source": [
    "test = test_clean_offensive['tweet'].values.tolist()\n"
   ]
  },
  {
   "source": [
    "## Frequency of words on non-cleaned texts"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bag_test = bag_of_words(words_test_txt)\n",
    "bag_train = bag_of_words(words_train_txt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "freq_test = freq_words(bag_test)\n",
    "freq_train = freq_words(bag_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "TweetEval Tutorial",
   "provenance": []
  },
  "environment": {
   "name": "tf2-gpu.2-1.m46",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m46"
  },
  "kernelspec": {
   "name": "python376jvsc74a57bd07ca953050fbd6db0e15562356b1a786d9418e582a94adc67b248dc4dbecd989f",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "02f91bc3545b48808e4812d13f480875": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_25a58cab43ef453d8b9de797925f32fa",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_db546face47140a59dcfc21539492c51",
      "value": " 150/150 [00:00&lt;00:00, 233B/s]"
     }
    },
    "0a1beafebb954a498d7a1cccc7cee445": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1e5e9065e1ed4adbb1b451672ca4e793": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2151898d3e1c47de9e681c8b7b8779e4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_627335e621884fb6957b7b6731339148",
       "IPY_MODEL_02f91bc3545b48808e4812d13f480875"
      ],
      "layout": "IPY_MODEL_6e236c3f5e084a108811f1e6a32cb990"
     }
    },
    "25a58cab43ef453d8b9de797925f32fa": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2adcfba8e99b4ceeae8cea6c4060c58d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2b6e1410b17543afb5dbf619666e83bd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "3b1773e755ca402f98b5033ed4af1b7a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "41b12ad69a684cb597594e60355253ae": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "498473ac68aa48c1ba4d5578543a8e2b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "627335e621884fb6957b7b6731339148": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0a1beafebb954a498d7a1cccc7cee445",
      "max": 150,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2b6e1410b17543afb5dbf619666e83bd",
      "value": 150
     }
    },
    "683e01b6cd98402783fe062f75177b68": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6d41343536e54dd3804ace50e15b7953": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6e236c3f5e084a108811f1e6a32cb990": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7045c78b1b164511b62e20bec70f1639": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "75528dba634e465992e5b52107f19032": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3b1773e755ca402f98b5033ed4af1b7a",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_eab919bdf6cb499b90a14c297bbc94dc",
      "value": " 456k/456k [00:01&lt;00:00, 251kB/s]"
     }
    },
    "77dad5843dbc43eab577b4cef273228d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ca9214bf42bf49d99bcec60cc60c6e9c",
      "max": 498682569,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7045c78b1b164511b62e20bec70f1639",
      "value": 498682569
     }
    },
    "7a80cfb9bfda407c9ee1b9bd8b11a2f5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7de2e7e4e8f44dccabc908334f7d1e21": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "7ede88ead0d449a18ca343d47b723e87": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "88a7b0479e8c40b6aea4084f8039d4f6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "8c7d3e2bd2714e929d6859e15e83003f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "91abbefb5e734d47a01158aebf46d7ce": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "9655bb1c674342cfb3ec38378a376729": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_41b12ad69a684cb597594e60355253ae",
      "max": 779,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_88a7b0479e8c40b6aea4084f8039d4f6",
      "value": 779
     }
    },
    "a5c3c70d67124cd09894d3b52dc33e16": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_683e01b6cd98402783fe062f75177b68",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_e7aee59723a5469a9691322d6e124226",
      "value": " 779/779 [00:00&lt;00:00, 974B/s]"
     }
    },
    "b2d0e13e096c49cd9e25bb8a526b1ef7": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b9024a2af0984f0a99d7542077d5d183": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_77dad5843dbc43eab577b4cef273228d",
       "IPY_MODEL_d647beae5bff452ebc5e4dbc9974d63e"
      ],
      "layout": "IPY_MODEL_2adcfba8e99b4ceeae8cea6c4060c58d"
     }
    },
    "c2d4ce2a084b4f19bf7ab624c0a853ea": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fd2b26c80b8e4807a595ce1ff9881623",
      "max": 456318,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7de2e7e4e8f44dccabc908334f7d1e21",
      "value": 456318
     }
    },
    "ca9214bf42bf49d99bcec60cc60c6e9c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d647beae5bff452ebc5e4dbc9974d63e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1e5e9065e1ed4adbb1b451672ca4e793",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_6d41343536e54dd3804ace50e15b7953",
      "value": " 499M/499M [00:08&lt;00:00, 61.0MB/s]"
     }
    },
    "db546face47140a59dcfc21539492c51": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "df23412503a1413aadacd254b573c315": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e0847fcebda84c4d8b00b27234d35bc4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9655bb1c674342cfb3ec38378a376729",
       "IPY_MODEL_a5c3c70d67124cd09894d3b52dc33e16"
      ],
      "layout": "IPY_MODEL_8c7d3e2bd2714e929d6859e15e83003f"
     }
    },
    "e2f226a69f704168bbbc6fca9df15e23": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b2d0e13e096c49cd9e25bb8a526b1ef7",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_7a80cfb9bfda407c9ee1b9bd8b11a2f5",
      "value": " 899k/899k [00:02&lt;00:00, 323kB/s]"
     }
    },
    "e7aee59723a5469a9691322d6e124226": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e958eee9b3db48abae24a1f8be1b5d91": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c2d4ce2a084b4f19bf7ab624c0a853ea",
       "IPY_MODEL_75528dba634e465992e5b52107f19032"
      ],
      "layout": "IPY_MODEL_498473ac68aa48c1ba4d5578543a8e2b"
     }
    },
    "eab919bdf6cb499b90a14c297bbc94dc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f55b841175a54f7ea34306caf6d775cd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_df23412503a1413aadacd254b573c315",
      "max": 898822,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_91abbefb5e734d47a01158aebf46d7ce",
      "value": 898822
     }
    },
    "f9c4c6ab3ba449e199d7035bff07a5db": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f55b841175a54f7ea34306caf6d775cd",
       "IPY_MODEL_e2f226a69f704168bbbc6fca9df15e23"
      ],
      "layout": "IPY_MODEL_7ede88ead0d449a18ca343d47b723e87"
     }
    },
    "fd2b26c80b8e4807a595ce1ff9881623": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  },
  "metadata": {
   "interpreter": {
    "hash": "7ca953050fbd6db0e15562356b1a786d9418e582a94adc67b248dc4dbecd989f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}