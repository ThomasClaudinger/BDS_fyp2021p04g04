{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combined function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics of the whole dataset\n",
    "def dataset_statistics(dataset, title, path):\n",
    "    cleaned_words = del_punctuations(dataset)\n",
    "    token = tokenization(cleaned_words, dataset)\n",
    "    cleaned_words = stop_words(token)\n",
    "    freq_words = word_frequency(cleaned_words)\n",
    "    top_20 = top_20_most_common_words(freq_words)\n",
    "    least_words = least_common_words(freq_words)\n",
    "    \n",
    "    print(\"Statistics for \" + title + \" dataset:\\n\")\n",
    "    #print('No punctuation:\\n',cleaned_words[:20],'\\n')\n",
    "    #print(f'Number of words in tokenization: {len(token)}')\n",
    "    #print(f'Number of words after removing Stop Words: {len(cleaned_words)}')\n",
    "    #print(\"Number of total words in the vocabulary \", len(freq_words), \"\\n\")\n",
    "    print('Top 20 in freq_words \\n',top_20, '\\n')\n",
    "    print('Top 20 least common words in freq_words \\n',least_words[0:20], '\\n')\n",
    "\n",
    "    # Tokens statistics and ratio:\n",
    "    _, tokens, _, _,unique_tokens, _ ,_ = basic_statistics(dataset,printer = True)\n",
    "\n",
    "    # LogLog plot of words\n",
    "    print(\"Zipf's law, LogLog Plot\")\n",
    "    llplot([freq_words], labels=[\"all\"], title=\"Cleaned \" + title + \" dataset, loglog plot\")\n",
    "    \n",
    "    # Plot of True labels distribution\n",
    "    print(\"Distribution of true labels:\\n\")\n",
    "    test_path = path + \"test_labels.txt\"\n",
    "    val_path = path + \"val_labels.txt\"\n",
    "    train_path = path + \"train_labels.txt\"\n",
    "    map_path = path + \"mapping.txt\"\n",
    "\n",
    "    print(\"Plot of labels distribution:\")\n",
    "    test_labels = make_corpus(test_path) + make_corpus(val_path) + make_corpus(train_path)\n",
    "    test_labels = [int(i) for i in test_labels]\n",
    "    test_labels = dict(collections.Counter(test_labels))\n",
    "    test_labels = collections.OrderedDict(sorted(test_labels.items()))\n",
    "    plt.bar(test_labels.keys(), test_labels.values())\n",
    "    plt.show()\n",
    "    \n",
    "    #List of emoji labels\n",
    "    label_df = pd.read_csv(map_path, sep='\\t', lineterminator='\\n', header=None)\n",
    "    label_df = label_df.iloc[: , :-1]\n",
    "    label_df = label_df.rename(columns={0:\"Label no.\",1:\"Emoji\", 2:\"Emoji as txt\"})\n",
    "    label_df[\"Count\"] = test_labels.values()\n",
    "    label_df[\"Percentage\"] = round(label_df[\"Count\"]/label_df[\"Count\"].sum()*100,2)\n",
    "\n",
    "    # Sorting by count\n",
    "    label_df.sort_values(\"Count\", ascending = False)\n",
    "    print(label_df)\n",
    "    \n",
    "    return None\n",
    "\n",
    "# For the whole Emoji dataset\n",
    "emoji_text = emoji_text_train + emoji_text_val + emoji_text_test\n",
    "dataset_statistics(emoji_text, \"whole Emoji\", \"datasets/emoji/\")\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "# For the whole Offensive dataset\n",
    "offensive_text = offensive_text_train + offensive_text_val + offensive_text_test\n",
    "dataset_statistics(offensive_text, \"whole Offensive\", \"datasets/offensive/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Barplot of emoji labels distribution\n",
    "test_path = \"datasets/emoji/test_labels.txt\"\n",
    "val_path = \"datasets/emoji/val_labels.txt\"\n",
    "train_path = \"datasets/emoji/train_labels.txt\"\n",
    "map_path = \"datasets/emoji/mapping.txt\"\n",
    "\n",
    "test_labels = make_corpus(test_path) + make_corpus(val_path) + make_corpus(train_path)\n",
    "test_labels = [int(i) for i in test_labels]\n",
    "test_labels = dict(collections.Counter(test_labels))\n",
    "test_labels = collections.OrderedDict(sorted(test_labels.items()))\n",
    "barplot = plt.bar(test_labels.keys(), test_labels.values())\n",
    "\n",
    "#List of emoji labels\n",
    "label_df = pd.read_csv(map_path, sep='\\t', lineterminator='\\n', header=None)\n",
    "label_df = label_df.drop([3], axis=1)\n",
    "label_df = label_df.rename(columns={0:\"Label no.\",1:\"Emoji\", 2:\"Emoji as txt\"})\n",
    "label_df[\"Count\"] = test_labels.values()\n",
    "label_df[\"Percentage\"] = round(label_df[\"Count\"]/label_df[\"Count\"].sum()*100,2)\n",
    "\n",
    "# Sorting by count\n",
    "label_df.sort_values(\"Count\", ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Offensive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Barplot of offensive labels distribution\n",
    "test_path = \"datasets/offensive/test_labels.txt\"\n",
    "val_path = \"datasets/offensive/val_labels.txt\"\n",
    "train_path = \"datasets/offensive/train_labels.txt\"\n",
    "map_path = \"datasets/offensive/mapping.txt\"\n",
    "\n",
    "test_labels = make_corpus(test_path) + make_corpus(val_path) + make_corpus(train_path)\n",
    "test_labels = [int(i) for i in test_labels]\n",
    "test_labels = dict(collections.Counter(test_labels))\n",
    "test_labels = collections.OrderedDict(sorted(test_labels.items()))\n",
    "plt.bar(test_labels.keys(), test_labels.values())\n",
    "\n",
    "#List of offensive labels\n",
    "label_df = pd.read_csv(map_path, sep='\\t', lineterminator='\\n', header=None)\n",
    "label_df = label_df.rename(columns={0:\"Label no.\",1:\"Label\"})\n",
    "label_df[\"Count\"] = test_labels.values()\n",
    "label_df[\"Percentage\"] = round(label_df[\"Count\"]/label_df[\"Count\"].sum()*100,2)\n",
    "label_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
