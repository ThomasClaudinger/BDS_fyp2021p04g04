{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'cat', 'sat', 'on', 'the', 'mat', 'His', 'name', 'was', 'Måns']\n",
      "['.', '.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "line = 'A cat sat on the mat. His name was Måns.'\n",
    "\n",
    "# Initialise lists\n",
    "tokens = []\n",
    "unmatchable = []\n",
    "\n",
    "# Compile patterns for speedup\n",
    "token_pat = re.compile(r'\\w+')\n",
    "skippable_pat = re.compile(r'\\s+')  # typically spaces\n",
    "\n",
    "# As long as there's any material left...\n",
    "while line:\n",
    "    # Try finding a skippable token delimiter first.\n",
    "    skippable_match = re.search(skippable_pat, line)\n",
    "    if skippable_match and skippable_match.start() == 0:\n",
    "        # If there is one at the beginning of the line, just skip it.\n",
    "        line = line[skippable_match.end():]\n",
    "    else:\n",
    "        # Else try finding a real token.\n",
    "        token_match = re.search(token_pat, line)\n",
    "        if token_match and token_match.start() == 0:\n",
    "            # If there is one at the beginning of the line, tokenise it.\n",
    "            tokens.append(line[:token_match.end()])\n",
    "            line = line[token_match.end():]\n",
    "        else:\n",
    "            # Else there is unmatchable material here.\n",
    "            # It ends where a skippable or token match starts, or at the end of the line.\n",
    "            unmatchable_end = len(line)\n",
    "            if skippable_match:\n",
    "                unmatchable_end = skippable_match.start()\n",
    "            if token_match:\n",
    "                unmatchable_end = min(unmatchable_end, token_match.start())\n",
    "            # Add it to unmatchable and discard from line.\n",
    "            unmatchable.append(line[:unmatchable_end])\n",
    "            line = line[unmatchable_end:]\n",
    "\n",
    "print(tokens)\n",
    "print(unmatchable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Heroin', 'is', 'my', 'passion', '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This is the ideal tokenization\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer()\n",
    "\n",
    "sentence = 'Heroin is my passion.'\n",
    "\n",
    "tknzr.tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recreate\n",
    "import re\n",
    "import pathlib\n",
    "\n",
    "file_path_train = pathlib.Path('datasets/offensive/train_text.txt')\n",
    "file_path_validation = pathlib.Path('datasets/offensive/val_text.txt')\n",
    "\n",
    "token_pattern = re.compile(r'\\w+')\n",
    "\n",
    "with open(file_path_train, 'r') as f:\n",
    "    line = f.readline()\n",
    "    tokens = []\n",
    "    corpus_train =[]\n",
    "    while line:\n",
    "        line = f.readline()\n",
    "#         print(line)\n",
    "#         print(\"OUR beta tokenizer\",re.findall(token_pattern,line))\n",
    "#         print(\"DESIRED  tokenizer\",tknzr.tokenize(line))\n",
    "#         print('\\n')\n",
    "        corpus_train.append(tknzr.tokenize(line))\n",
    "    \n",
    "\n",
    "\n",
    "with open(file_path_validation, 'r') as f:\n",
    "    line = f.readline()\n",
    "    tokens = []\n",
    "    corpus_val =[]\n",
    "    while line:\n",
    "        line = f.readline()\n",
    "#         print(line)\n",
    "#         print(\"OUR beta tokenizer\",re.findall(token_pattern,line))\n",
    "#         print(\"DESIRED  tokenizer\",tknzr.tokenize(line))\n",
    "#         print('\\n')\n",
    "        corpus_val.append(tknzr.tokenize(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('@user', 'Eight'),\n",
       " ('Eight', 'years'),\n",
       " ('years', 'the'),\n",
       " ('the', 'republicans'),\n",
       " ('republicans', 'denied'),\n",
       " ('denied', 'obama'),\n",
       " ('obama', '’'),\n",
       " ('’', 's'),\n",
       " ('s', 'picks'),\n",
       " ('picks', '.'),\n",
       " ('.', 'Breitbarters'),\n",
       " ('Breitbarters', 'outrage'),\n",
       " ('outrage', 'is'),\n",
       " ('is', 'as'),\n",
       " ('as', 'phony'),\n",
       " ('phony', 'as'),\n",
       " ('as', 'their'),\n",
       " ('their', 'fake'),\n",
       " ('fake', 'president'),\n",
       " ('president', '.')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.util import bigrams\n",
    "#\"Neibourghood of a word of size 2\"\n",
    "list(bigrams(corpus_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<s>', '@user'),\n",
       " ('@user', 'Eight'),\n",
       " ('Eight', 'years'),\n",
       " ('years', 'the'),\n",
       " ('the', 'republicans'),\n",
       " ('republicans', 'denied'),\n",
       " ('denied', 'obama'),\n",
       " ('obama', '’'),\n",
       " ('’', 's'),\n",
       " ('s', 'picks'),\n",
       " ('picks', '.'),\n",
       " ('.', 'Breitbarters'),\n",
       " ('Breitbarters', 'outrage'),\n",
       " ('outrage', 'is'),\n",
       " ('is', 'as'),\n",
       " ('as', 'phony'),\n",
       " ('phony', 'as'),\n",
       " ('as', 'their'),\n",
       " ('their', 'fake'),\n",
       " ('fake', 'president'),\n",
       " ('president', '.'),\n",
       " ('.', '</s>')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "#Now we are not skipping the first word and the last word and we know that they are at the beginning of a sentence and the end of a sentence\n",
    "list(pad_both_ends(corpus_train[0], n=2))\n",
    "\n",
    "list(bigrams(pad_both_ends(corpus_train[0], n=2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<s>',),\n",
       " ('<s>', '@user'),\n",
       " ('@user',),\n",
       " ('@user', 'Eight'),\n",
       " ('Eight',),\n",
       " ('Eight', 'years'),\n",
       " ('years',),\n",
       " ('years', 'the'),\n",
       " ('the',),\n",
       " ('the', 'republicans'),\n",
       " ('republicans',),\n",
       " ('republicans', 'denied'),\n",
       " ('denied',),\n",
       " ('denied', 'obama'),\n",
       " ('obama',),\n",
       " ('obama', '’'),\n",
       " ('’',),\n",
       " ('’', 's'),\n",
       " ('s',),\n",
       " ('s', 'picks'),\n",
       " ('picks',),\n",
       " ('picks', '.'),\n",
       " ('.',),\n",
       " ('.', 'Breitbarters'),\n",
       " ('Breitbarters',),\n",
       " ('Breitbarters', 'outrage'),\n",
       " ('outrage',),\n",
       " ('outrage', 'is'),\n",
       " ('is',),\n",
       " ('is', 'as'),\n",
       " ('as',),\n",
       " ('as', 'phony'),\n",
       " ('phony',),\n",
       " ('phony', 'as'),\n",
       " ('as',),\n",
       " ('as', 'their'),\n",
       " ('their',),\n",
       " ('their', 'fake'),\n",
       " ('fake',),\n",
       " ('fake', 'president'),\n",
       " ('president',),\n",
       " ('president', '.'),\n",
       " ('.',),\n",
       " ('.', '</s>'),\n",
       " ('</s>',)]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.util import everygrams\n",
    "#Know we can do whatever n-gram we want by changing max_len\n",
    "padded_bigrams = list(pad_both_ends(corpus_train[0], n=2))\n",
    "list(everygrams(padded_bigrams, max_len=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "325075"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a vocabulary\n",
    "from nltk.lm.preprocessing import flatten\n",
    "voc = list(flatten(pad_both_ends(sent, n=2) for sent in corpus_train))\n",
    "\n",
    "len(voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "\n",
    "train, vocab = padded_everygram_pipeline(2, corpus_train)\n",
    "\n",
    "#vocab is the same as voc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train our model\n",
    "from nltk.lm import MLE\n",
    "lm = MLE(2) #2 is the highest n-gram\n",
    "\n",
    "lm.fit(train,vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.vocab.lookup('unjj')\n",
    "lm.vocab.lookup('the')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<NgramCounter with 2 ngram orders and 638234 ngrams>\n"
     ]
    }
   ],
   "source": [
    "#Count up the ngrams from the training corpus\n",
    "\n",
    "print(lm.counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7268"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.counts['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.022357917403676073"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#How probable are words in certain contexts (relative frequency)\n",
    "\n",
    "lm.score('the')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03650793650793651"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#What is the chance that 'the' is preceded by 'have'\n",
    "\n",
    "lm.score('the',['have'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5.483070379559561"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To avoid underflow we take logarithm\n",
    "\n",
    "lm.logscore(\"the\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Customs', 'Fees', '.', 'Guess', 'she']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Generate random words from our vocabulary\n",
    "lm.generate(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 try different models and count n-grams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<NgramCounter with 2 ngram orders and 638234 ngrams>\n"
     ]
    }
   ],
   "source": [
    "#2 gram order\n",
    "train, vocab = padded_everygram_pipeline(2, corpus_train)\n",
    "lm2 = MLE(2) #2 is the highest n-gram\n",
    "\n",
    "lm2.fit(train,vocab)\n",
    "print(lm2.counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<NgramCounter with 3 ngram orders and 1010973 ngrams>\n"
     ]
    }
   ],
   "source": [
    "#3 gram order\n",
    "train, vocab = padded_everygram_pipeline(3, corpus_train)\n",
    "lm3 = MLE(3)\n",
    "lm3.fit(train,vocab)\n",
    "print(lm3.counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<NgramCounter with 4 ngram orders and 1419460 ngrams>\n"
     ]
    }
   ],
   "source": [
    "#4 gram order\n",
    "train, vocab = padded_everygram_pipeline(4, corpus_train)\n",
    "lm4 = MLE(4)\n",
    "lm4.fit(train,vocab)\n",
    "print(lm4.counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anybody', 'who', 'gave', 'it', 'was']"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm2.generate(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['GUILTY', 'of', 'CRIMINAL', 'BEHAVIOR', '&']"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm3.generate(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['that', \"doesn't\", 'mean', 'we', 'want']"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm4.generate(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.022357917403676073"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm2.score('the')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02083076579145732"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm3.score('the')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01949889869318746"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm4.score('the')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inf"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm2.perplexity([('a', 'b'), ('c', 'd')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotation agreements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#lukas annotation\n",
    "rater1=[]\n",
    "with open('annotation/lukas.txt', 'r') as f:\n",
    "    line = f.readline()\n",
    "    while line:\n",
    "        rater1.append(line.strip())\n",
    "        line = f.readline()\n",
    "\n",
    "\n",
    "\n",
    "#thomas annotation\n",
    "rater2=[]\n",
    "with open('annotation/thoma.txt', 'r') as f:\n",
    "    line = f.readline()\n",
    "    while line:\n",
    "        rater2.append(line.strip())\n",
    "        line = f.readline()\n",
    "\n",
    "#martin annotation\n",
    "rater3=[]\n",
    "with open('annotation/martin.txt', 'r') as f:\n",
    "    line = f.readline()\n",
    "    while line:\n",
    "        rater3.append(line.strip())\n",
    "        line = f.readline()\n",
    "#yasmine annotation\n",
    "rater4=[]\n",
    "with open('annotation/yasmine.txt', 'r') as f:\n",
    "    line = f.readline()\n",
    "    while line:\n",
    "        rater4.append(line.strip().split()[1])\n",
    "        line = f.readline()\n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#len(rater1)\n",
    "#len(rater2)\n",
    "#len(rater3)\n",
    "#len(rater4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kappa 0.43337682467527944\n",
      "fleiss 0.41626129256428096\n",
      "alpha 0.39438657407407396\n",
      "scotts 0.39236111111111127\n",
      "avg Ao 0.7200000000000001\n"
     ]
    }
   ],
   "source": [
    "from nltk import agreement\n",
    "rater1 = rater1[0:100]\n",
    "rater2 = rater2[0:100]\n",
    "rater3 = rater3[0:100]\n",
    "rater4 = rater4[0:100]\n",
    "\n",
    "taskdata=[[0,str(i),str(rater1[i])] for i in range(0,len(rater1))]+[[1,str(i),str(rater2[i])] for i in range(0,len(rater2))]+[[2,str(i),str(rater3[i])] for i in range(0,len(rater3))]#+[[3,str(i),str(rater4[i])] for i in range(0,len(rater4))]\n",
    "\n",
    "\n",
    "ratingtask = agreement.AnnotationTask(data=taskdata)\n",
    "\n",
    "#Cohen 1960 Averages naively over kappas for each coder pair.\n",
    "print(\"kappa \" +str(ratingtask.kappa()))\n",
    "print(\"fleiss \" + str(ratingtask.multi_kappa()))\n",
    "print(\"alpha \" +str(ratingtask.alpha()))\n",
    "print(\"scotts \" + str(ratingtask.pi()))\n",
    "\n",
    "#Average observed agreement across all coders and items\n",
    "print('avg Ao', ratingtask.avg_Ao())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
