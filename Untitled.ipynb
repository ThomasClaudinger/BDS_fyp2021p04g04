{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "from keras.datasets import mnist\n",
    "#download mnist data and split into train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "#reshape data to fit model\n",
    "X_train = X_train.reshape(60000,28,28,1)\n",
    "X_test = X_test.reshape(10000,28,28,1)\n",
    "from keras.utils import to_categorical\n",
    "#one-hot encode target column\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten\n",
    "#create model\n",
    "model = Sequential()\n",
    "#add model layers\n",
    "model.add(Conv2D(64, kernel_size=3, activation=\"relu\", input_shape=(28,28,1)))\n",
    "model.add(Conv2D(32, kernel_size=3, activation=\"relu\"))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "#compile model using accuracy to measure model performance\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "              #train the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3)\n",
    "model.predict(X_test[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Martin\\Anaconda3\\envs\\fyp04\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Martin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Martin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Martin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from gensim.models import Word2Vec\n",
    "from nltk import sent_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer #for word embedding\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score # bag of words\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import collections\n",
    "import csv\n",
    "import gensim\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import nltk.tokenize\n",
    "import numpy as np\n",
    "import operator\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import pickle\n",
    "import re, string\n",
    "import seaborn\n",
    "import string\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')#for model-building\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_dataset(dataframe):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(dataframe[\"tweet\"],dataframe      \n",
    "    [\"label\"],test_size=0.2,shuffle=True)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euc_dist(x1, x2):\n",
    "    return np.sqrt(np.sum((x1-x2)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merging_labels_and_sentences(textfile, labelfile):\n",
    "\n",
    "    # read textfile and labelfile into two separate dataframes\n",
    "    df_text = pd.read_csv(textfile + '.txt', header=None, skiprows = 0,\n",
    "        names=['tweet'], sep='\\t', quoting=3)\n",
    "    df_labels = pd.read_csv(labelfile + '.txt',header=None, skiprows = 0, names=['label'],\n",
    "        sep='\\t', quoting=3)\n",
    "\n",
    "    index_text = [x for x in range(1, len(df_text.values)+1)]\n",
    "\n",
    "    df_labels.insert(loc=0, column='id', value =index_text)\n",
    "    df_text.insert(loc=0, column='id', value =index_text)\n",
    "    final_df = df_text.merge(df_labels, on='id', how='left')\n",
    "\n",
    "    return final_df\n",
    "\n",
    "\n",
    "#building Word2Vec model\n",
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = len(next(iter(word2vec.values())))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([np.mean([self.word2vec[w] \n",
    "        for w in words if w in self.word2vec] or [np.zeros(self.dim)], axis=0) \n",
    "        for words in X])\n",
    "\n",
    "#building Word2Vec model\n",
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = len(next(iter(word2vec.values())))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([np.mean([self.word2vec[w] \n",
    "        for w in words if w in self.word2vec] or [np.zeros(self.dim)], axis=0) \n",
    "        for words in X])\n",
    "def tokenize_train_test(X_train, X_test):\n",
    "    #Word2Vec\n",
    "    # Word2Vec runs on tokenized sentences\n",
    "    X_train_tok= [nltk.word_tokenize(i) for i in X_train]  \n",
    "    X_test_tok= [nltk.word_tokenize(i) for i in X_test]\n",
    "\n",
    "    return X_train_tok, X_test_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#download mnist data and split into train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "#reshape data to fit model\n",
    "X_train = X_train.reshape(60000,28,28,1)\n",
    "X_test = X_test.reshape(10000,28,28,1)\n",
    "\n",
    "#one-hot encode target column\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create model\n",
    "model = Sequential()\n",
    "#add model layers\n",
    "model.add(Conv2D(64, kernel_size=3, activation=\"relu\", input_shape=(28,28,1)))\n",
    "model.add(Conv2D(32, kernel_size=3, activation=\"relu\"))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = merging_labels_and_sentences('datasets/offensive/val_text', 'datasets/offensive/val_labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are splitting the data into training and test\n",
    "X_train, X_test, y_train, y_test = split_dataset(val_df)\n",
    "\n",
    "# Here we tokenize the data\n",
    "X_train_tok, X_test_tok = tokenize_train_test(X_train, X_test)\n",
    "\n",
    "# (tf-Idf)\n",
    "tfidf_vectorizer = TfidfVectorizer(use_idf=True)\n",
    "X_train_vectors_tfidf = tfidf_vectorizer.fit_transform(X_train) \n",
    "X_test_vectors_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "\n",
    "val_df['clean_text_tok']=[nltk.word_tokenize(i) for i in val_df['tweet']]\n",
    "model = gensim.models.Word2Vec(val_df['clean_text_tok'],min_count=1)\n",
    "w2v = dict(zip(model.wv.index_to_key, model.wv))      \n",
    "modelw = MeanEmbeddingVectorizer(w2v)\n",
    "\n",
    "# converting text to numerical data using Word2Vec\n",
    "X_train_vectors_w2v = modelw.transform(X_train_tok)\n",
    "X_test_vectors_w2v = modelw.transform(X_test_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train\n",
    "li = []\n",
    "for elm in  X_train_vectors_w2v:\n",
    "    li.append(elm.reshape(10,10,1))\n",
    "X_train_vectors_w2v = np.array(li)\n",
    "something = abs(X_train_vectors_w2v)\n",
    "maximum = np.amax(something)\n",
    "X_train_vectors_w2v = np.divide(something,maximum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test\n",
    "li = []\n",
    "for elm in  X_test_vectors_w2v:\n",
    "    li.append(elm.reshape(10,10,1))\n",
    "X_test_vectors_w2v = np.array(li)\n",
    "something = abs(X_test_vectors_w2v)\n",
    "maximum = np.amax(something)\n",
    "X_test_vectors_w2v = np.divide(something,maximum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.12246324],\n",
       "       [0.12119006],\n",
       "       [0.11462697],\n",
       "       [0.05310346],\n",
       "       [0.05954191],\n",
       "       [0.3456635 ],\n",
       "       [0.23062675],\n",
       "       [0.5246075 ],\n",
       "       [0.19759466],\n",
       "       [0.2857701 ]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vectors_w2v[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.15647037],\n",
       "       [0.1490343 ],\n",
       "       [0.14796902],\n",
       "       [0.08470155],\n",
       "       [0.06712216],\n",
       "       [0.42500442],\n",
       "       [0.28412497],\n",
       "       [0.6361573 ],\n",
       "       [0.23912083],\n",
       "       [0.35506833]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_vectors_w2v[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1.], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0.], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 8, 8, 10)          100       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 6, 6, 5)           455       \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 180)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 362       \n",
      "=================================================================\n",
      "Total params: 917\n",
      "Trainable params: 917\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#create model\n",
    "model = Sequential()\n",
    "#add model layers\n",
    "model.add(Conv2D(10, kernel_size=3, activation=\"relu\", input_shape=(10,10,1)))\n",
    "model.add(Conv2D(5, kernel_size=3, activation=\"relu\"))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(2, activation=\"softmax\"))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "34/34 [==============================] - 1s 9ms/step - loss: 0.6645 - accuracy: 0.6553 - val_loss: 0.6504 - val_accuracy: 0.6453\n",
      "Epoch 2/3\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.6413 - accuracy: 0.6553 - val_loss: 0.6494 - val_accuracy: 0.6453\n",
      "Epoch 3/3\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.6381 - accuracy: 0.6553 - val_loss: 0.6502 - val_accuracy: 0.6453\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1f80b899808>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train the model\n",
    "model.fit(X_train_vectors_w2v, y_train, validation_data=(X_test_vectors_w2v, y_test), epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = merging_labels_and_sentences('datasets/emoji/train_text', 'datasets/emoji/train_labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are splitting the data into training and test\n",
    "X_train, X_test, y_train, y_test = split_dataset(val_df)\n",
    "\n",
    "# Here we tokenize the data\n",
    "X_train_tok, X_test_tok = tokenize_train_test(X_train, X_test)\n",
    "\n",
    "# (tf-Idf)\n",
    "tfidf_vectorizer = TfidfVectorizer(use_idf=True)\n",
    "X_train_vectors_tfidf = tfidf_vectorizer.fit_transform(X_train) \n",
    "X_test_vectors_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "\n",
    "val_df['clean_text_tok']=[nltk.word_tokenize(i) for i in val_df['tweet']]\n",
    "model = gensim.models.Word2Vec(val_df['clean_text_tok'],min_count=1)\n",
    "w2v = dict(zip(model.wv.index_to_key, model.wv))      \n",
    "modelw = MeanEmbeddingVectorizer(w2v)\n",
    "\n",
    "# converting text to numerical data using Word2Vec\n",
    "X_train_vectors_w2v = modelw.transform(X_train_tok)\n",
    "X_test_vectors_w2v = modelw.transform(X_test_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train\n",
    "li = []\n",
    "for elm in  X_train_vectors_w2v:\n",
    "    li.append(elm.reshape(10,10,1))\n",
    "X_train_vectors_w2v = np.array(li)\n",
    "something = abs(X_train_vectors_w2v)\n",
    "maximum = np.amax(something)\n",
    "X_train_vectors_w2v = np.divide(something,maximum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test\n",
    "li = []\n",
    "for elm in  X_test_vectors_w2v:\n",
    "    li.append(elm.reshape(10,10,1))\n",
    "X_test_vectors_w2v = np.array(li)\n",
    "something = abs(X_test_vectors_w2v)\n",
    "maximum = np.amax(something)\n",
    "X_test_vectors_w2v = np.divide(something,maximum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "       0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 8, 8, 64)          640       \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 6, 6, 32)          18464     \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 20)                23060     \n",
      "=================================================================\n",
      "Total params: 42,164\n",
      "Trainable params: 42,164\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#create model\n",
    "sigurt = Sequential()\n",
    "#add model layers\n",
    "sigurt.add(Conv2D(64, kernel_size=3, activation=\"relu\", input_shape=(10,10,1)))\n",
    "sigurt.add(Conv2D(32, kernel_size=3, activation=\"relu\"))\n",
    "sigurt.add(Flatten())\n",
    "sigurt.add(Dense(20, activation=\"softmax\"))\n",
    "#compile model using accuracy to measure model performance\n",
    "sigurt.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "sigurt.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1125/1125 [==============================] - 9s 8ms/step - loss: 2.5658 - accuracy: 0.2560 - val_loss: 2.4846 - val_accuracy: 0.2689\n",
      "Epoch 2/3\n",
      "1125/1125 [==============================] - 8s 7ms/step - loss: 2.4197 - accuracy: 0.2875 - val_loss: 2.4253 - val_accuracy: 0.2789\n",
      "Epoch 3/3\n",
      "1125/1125 [==============================] - 7s 6ms/step - loss: 2.3680 - accuracy: 0.2991 - val_loss: 2.3961 - val_accuracy: 0.2906\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1f80b53b748>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train the model\n",
    "sigurt.fit(X_train_vectors_w2v, y_train, validation_data=(X_test_vectors_w2v, y_test), epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9000, 20)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9000, 10, 10, 1)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_vectors_w2v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred = np.array([ np.argmax(x) for x in sigurt.predict(X_test_vectors_w2v)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.array([ np.argmax(x) for x in y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.79      0.59      1812\n",
      "           1       0.16      0.33      0.21       978\n",
      "           2       0.22      0.50      0.31       963\n",
      "           3       0.12      0.00      0.00       432\n",
      "           4       0.23      0.01      0.03       402\n",
      "           5       0.08      0.00      0.01       459\n",
      "           6       0.05      0.00      0.00       381\n",
      "           7       0.13      0.12      0.12       458\n",
      "           8       0.00      0.00      0.00       256\n",
      "           9       0.20      0.04      0.07       283\n",
      "          10       0.22      0.50      0.30       376\n",
      "          11       0.26      0.11      0.15       190\n",
      "          12       0.58      0.03      0.05       256\n",
      "          13       0.00      0.00      0.00       219\n",
      "          14       0.00      0.00      0.00       227\n",
      "          15       0.00      0.00      0.00       172\n",
      "          16       0.00      0.00      0.00       309\n",
      "          17       0.44      0.32      0.37       284\n",
      "          18       0.25      0.01      0.01       295\n",
      "          19       0.00      0.00      0.00       248\n",
      "\n",
      "    accuracy                           0.29      9000\n",
      "   macro avg       0.17      0.14      0.11      9000\n",
      "weighted avg       0.22      0.29      0.21      9000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Martin\\Anaconda3\\envs\\fyp04\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Martin\\Anaconda3\\envs\\fyp04\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Martin\\Anaconda3\\envs\\fyp04\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_pred,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1424,  160,  146,    1,    1,    3,    4,   24,    0,    8,   30,\n",
       "           0,    0,    0,    0,    0,    0,   10,    1,    0],\n",
       "       [ 271,  324,  229,    2,    3,    0,    2,   52,    0,    0,   68,\n",
       "          11,    1,    0,    0,    0,    0,   14,    1,    0],\n",
       "       [ 114,  237,  486,    0,    4,    1,    2,   35,    0,    3,   65,\n",
       "           7,    0,    0,    0,    0,    0,    8,    1,    0],\n",
       "       [ 153,  110,   88,    1,    0,    4,    0,   30,    0,    5,   26,\n",
       "           5,    0,    0,    0,    0,    0,    9,    1,    0],\n",
       "       [  51,  118,  131,    1,    6,    1,    2,   16,    0,    1,   68,\n",
       "           5,    0,    0,    0,    0,    0,    2,    0,    0],\n",
       "       [  95,  146,  151,    1,    2,    2,    0,   21,    0,    5,   24,\n",
       "           3,    0,    0,    0,    0,    0,    9,    0,    0],\n",
       "       [  76,  132,   96,    0,    1,    0,    1,   28,    0,    3,   31,\n",
       "           3,    0,    0,    0,    0,    1,    8,    1,    0],\n",
       "       [  80,  125,  141,    1,    1,    3,    0,   53,    0,    1,   42,\n",
       "           3,    0,    0,    0,    0,    0,    8,    0,    0],\n",
       "       [  95,   68,   49,    0,    1,    2,    2,   13,    0,    1,   17,\n",
       "           3,    1,    0,    0,    0,    0,    4,    0,    0],\n",
       "       [  87,   74,   57,    0,    0,    2,    0,   11,    0,   11,   29,\n",
       "           2,    1,    0,    0,    0,    0,    8,    1,    0],\n",
       "       [  26,   75,   64,    0,    1,    1,    0,   11,    0,    1,  187,\n",
       "           3,    0,    0,    0,    0,    0,    7,    0,    0],\n",
       "       [  31,   67,   31,    0,    0,    2,    1,   19,    0,    0,   14,\n",
       "          21,    1,    0,    0,    0,    0,    3,    0,    0],\n",
       "       [ 209,   16,   16,    0,    0,    0,    0,    2,    0,    0,    2,\n",
       "           2,    7,    0,    0,    0,    0,    2,    0,    0],\n",
       "       [  77,   65,   38,    1,    0,    0,    0,    9,    0,    2,   21,\n",
       "           2,    0,    0,    0,    0,    0,    4,    0,    0],\n",
       "       [  50,   50,   95,    0,    0,    0,    0,    6,    0,    1,   19,\n",
       "           2,    0,    0,    0,    0,    1,    3,    0,    0],\n",
       "       [  32,   34,   62,    0,    3,    0,    1,   11,    0,    0,   23,\n",
       "           3,    0,    0,    0,    0,    0,    3,    0,    0],\n",
       "       [  65,   99,  102,    0,    0,    2,    2,   14,    0,    4,   15,\n",
       "           1,    1,    0,    0,    0,    0,    4,    0,    0],\n",
       "       [  38,   71,   35,    0,    1,    1,    2,   21,    0,    5,   18,\n",
       "           2,    0,    0,    0,    0,    0,   90,    0,    0],\n",
       "       [  20,   48,   62,    0,    2,    0,    1,    3,    0,    4,  146,\n",
       "           3,    0,    0,    0,    0,    0,    4,    2,    0],\n",
       "       [  37,   70,  106,    0,    0,    0,    2,   15,    0,    0,   12,\n",
       "           1,    0,    0,    0,    0,    0,    5,    0,    0]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_pred,pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
